{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "def show(imgs):\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = transforms.ToPILImage()(denormalize(img).to('cpu'))\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def denormalize(img):\n",
    "    return img.to('cpu') * torch.Tensor([0.229, 0.224, 0.225]).view((3,1,1)) + torch.tensor([[0.485, 0.456, 0.406]]).view((3,1,1))\n",
    "\n",
    "def prepare_plots(attr, img):\n",
    "    img = denormalize(img).numpy().transpose((1,2,0)) * 255\n",
    "    attr = attr.to('cpu').numpy().transpose((1,2,0))\n",
    "    return attr, img\n",
    "\n",
    "# Download an example image from the pytorch website\n",
    "filename = \"../data/dog.jpg\"\n",
    "if (not os.path.exists(filename)):\n",
    "    import urllib\n",
    "    url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", filename)\n",
    "    try: urllib.URLopener().retrieve(url, filename)\n",
    "    except: urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "image = Image.open(filename)\n",
    "input_image = preprocess(image)\n",
    "\n",
    "# Read the categories\n",
    "with open(\"../data/imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/n01443537_goldfish.JPEG\n",
      "../data/n01532829_house_finch.JPEG\n",
      "../data/n01491361_tiger_shark.JPEG\n",
      "../data/n01622779_great_grey_owl.JPEG\n",
      "../data/n01693334_green_lizard.JPEG\n",
      "../data/n03662601_lifeboat.JPEG\n",
      "../data/n03666591_lighter.JPEG\n",
      "../data/n03691459_loudspeaker.JPEG\n",
      "../data/n01742172_boa_constrictor.JPEG\n",
      "../data/n01744401_rock_python.JPEG\n",
      "../data/n01773797_garden_spider.JPEG\n"
     ]
    }
   ],
   "source": [
    "# load other images\n",
    "# https://github.com/EliSchwartz/imagenet-sample-images\n",
    "    \n",
    "def load_img(url):\n",
    "    filename = \"../data/\" + url.split(\"/\")[-1]\n",
    "    print(filename)\n",
    "    if (not os.path.exists(filename)):\n",
    "        import urllib\n",
    "        try: urllib.URLopener().retrieve(url, filename)\n",
    "        except: urllib.request.urlretrieve(url, filename)\n",
    "    image = Image.open(filename)\n",
    "    input_image = preprocess(image)\n",
    "    return input_image\n",
    "\n",
    "git_path = \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/\"\n",
    "\n",
    "examples = [\n",
    "    \"n01443537_goldfish.JPEG\",\n",
    "    \"n01532829_house_finch.JPEG\",\n",
    "    \"n01491361_tiger_shark.JPEG\",\n",
    "    \"n01622779_great_grey_owl.JPEG\",\n",
    "    \"n01693334_green_lizard.JPEG\",\n",
    "    \"n03662601_lifeboat.JPEG\",\n",
    "    \"n03666591_lighter.JPEG\",\n",
    "    \"n03691459_loudspeaker.JPEG\",\n",
    "    \"n01742172_boa_constrictor.JPEG\",\n",
    "    \"n01744401_rock_python.JPEG\",\n",
    "    \"n01773797_garden_spider.JPEG\",\n",
    "]\n",
    "\n",
    "other_images = [load_img(git_path + url) for url in examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "steps = 50\n",
    "\n",
    "input_batch = input_image.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "label = torch.LongTensor([categories.index(\"Samoyed\")] * (steps+1))\n",
    "\n",
    "baseline = 0 * input_batch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    baseline = baseline.to('cuda')\n",
    "    input_batch = input_batch.to('cuda')\n",
    "\n",
    "# Scale input and compute gradients.\n",
    "scaled_inputs = [baseline + (float(i)/steps)*(input_batch-baseline) for i in range(0, steps+1)]\n",
    "scaled_inputs = torch.cat(scaled_inputs)\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    scaled_inputs = scaled_inputs.to('cuda')\n",
    "    label = label.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "# inference\n",
    "scaled_inputs.requires_grad = True\n",
    "output = model(scaled_inputs)\n",
    "\n",
    "# get gradient for input\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion(output, label).backward()\n",
    "grads = scaled_inputs.grad\n",
    "\n",
    "# Use trapezoidal rule to approximate the integral.\n",
    "# See Section 4 of the following paper for an accuracy comparison between\n",
    "# left, right, and trapezoidal IG approximations:\n",
    "# \"Computing Linear Restrictions of Neural Networks\", Matthew Sotoudeh, Aditya V. Thakur\n",
    "# https://arxiv.org/abs/1908.06214\n",
    "grads = (grads[:-1] + grads[1:]) / 2.0\n",
    "avg_grads = grads.mean(0)\n",
    "integrated_gradients = (input_batch-baseline) * avg_grads  # shape: <inp.shape>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samoyed 0.8371971845626831\n",
      "white wolf 0.06099937856197357\n",
      "keeshond 0.039010610431432724\n",
      "Pomeranian 0.02333681285381317\n",
      "Arctic fox 0.013027846813201904\n"
     ]
    }
   ],
   "source": [
    "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[-1], dim=0)\n",
    "\n",
    "# Show top categories per image\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_pred(input_image, n=5):\n",
    "    input_batch = input_image.unsqueeze(0)\n",
    "    model.eval()\n",
    "    logits = model(input_batch)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    probsk = probs.topk(5)\n",
    "    topk = tuple((p,c, categories[c]) for p, c in zip(probsk[0][0].detach().numpy(), probsk[1][0].detach().numpy()))\n",
    "    return topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories.index(\"fireboat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topk_pred(input_image, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldm_feature_importance.VisualizationLibrary.visualization_lib import Visualize, show_pil_image, pil_image\n",
    "\n",
    "explanation = integrated_gradients[0]\n",
    "# explanation = (explanation - torch.mean(explanation))/torch.std(explanation, unbiased=False)\n",
    "a = pil_image(Visualize(\n",
    "    *prepare_plots(explanation, input_image),\n",
    "    polarity=\"both\",\n",
    "    clip_above_percentile=99,\n",
    "    clip_below_percentile=0,\n",
    "    overlay=True)).resize((v//4 for v in image.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VisualizationLibrary.visualization_lib import Visualize, show_pil_image, pil_image\n",
    "\n",
    "explanation = integrated_gradients[0]\n",
    "explanation = (explanation - torch.mean(explanation))/torch.std(explanation, unbiased=False)\n",
    "pil_image(Visualize(\n",
    "    *prepare_plots(explanation, input_image),\n",
    "    polarity=\"both\",\n",
    "    clip_above_percentile=99,\n",
    "    clip_below_percentile=0,\n",
    "    overlay=True)).resize((v//4 for v in image.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = integrated_gradients[0]\n",
    "explanation = (explanation - torch.mean(explanation))/torch.std(explanation, unbiased=False)\n",
    "input_image_w_gradient = input_image.clone()\n",
    "print(input_image.shape, explanation.shape)\n",
    "input_image_w_gradient[abs(explanation) < 0.1] = 0\n",
    "\n",
    "\n",
    "pil_image(Visualize(\n",
    "    *prepare_plots(explanation, input_image_w_gradient),\n",
    "    polarity=\"both\",\n",
    "    clip_above_percentile=99,\n",
    "    clip_below_percentile=0,\n",
    "    overlay=True)).resize((v//4 for v in image.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = integrated_gradients[0]\n",
    "input_image_w_gradient = input_image.clone()\n",
    "print(input_image.shape, explanation.shape)\n",
    "explanation = (explanation - torch.mean(explanation))/torch.std(explanation, unbiased=False)\n",
    "input_image_w_gradient[abs(explanation) < 0.01] = 0\n",
    "\n",
    "pil_image(Visualize(\n",
    "    *prepare_plots(explanation, input_image_w_gradient),\n",
    "    polarity=\"both\",\n",
    "    clip_above_percentile=99,\n",
    "    clip_below_percentile=0,\n",
    "    overlay=False)).resize((v//4 for v in image.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image):\n",
    "    output = model(image)\n",
    "    probabilities = torch.nn.functional.softmax(output[-1], dim=0)\n",
    "\n",
    "    # Show top categories per image\n",
    "    top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "    for i in range(top5_prob.size(0)):\n",
    "        print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topk_pred(input_image, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation = integrated_gradients[0]\n",
    "explanation = (explanation - torch.mean(explanation))/torch.std(explanation, unbiased=False)\n",
    "input_image_w_gradient = input_image.clone()\n",
    "input_image_w_gradient[abs(explanation) < 0.01] = 0\n",
    "\n",
    "get_topk_pred(input_image_w_gradient, n=5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2575392019334285e0602a4035eec46b9260ee4c95297ea34ade6e3c8b8fcaf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
